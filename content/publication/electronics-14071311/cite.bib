@article{electronics14071311,
 abstract = {As the demand for interpretable machine learning approaches continues to grow, there is an increasing necessity for human involvement in providing informative explanations for model decisions. This is necessary for building trust and transparency in AI-based systems, leading to the emergence of the Explainable Artificial Intelligence (XAI) field. Recently, a novel counterfactual explanation model, CELS, has been introduced. CELS learns a saliency map for the interests of an instance and generates a counterfactual explanation guided by the learned saliency map. While CELS represents the first attempt to exploit learned saliency maps not only to provide intuitive explanations for the reason behind the decision made by the time series classifier but also to explore post hoc counterfactual explanations, it exhibits limitations in terms of its high validity for the sake of ensuring high proximity and sparsity. In this paper, we present an enhanced approach that builds upon CELS. While the original model achieved promising results in terms of sparsity and proximity, it faced limitations in terms of validity. Our proposed method addresses this limitation by removing mask normalization to provide more informative and valid counterfactual explanations. Through extensive experimentation on datasets from various domains, we demonstrate that our approach outperforms the CELS model, achieving higher validity and producing more informative explanations.},
 article-number = {1311},
 author = {Li, Peiyu and Bahri, Omar and Hosseinzadeh, Pouya and Boubrahimi, Souka√Øna Filali and Hamdi, Shah Muhammad},
 doi = {10.3390/electronics14071311},
 issn = {2079-9292},
 journal = {Electronics},
 number = {7},
 title = {Info-CELS: Informative Saliency Map-Guided Counterfactual Explanation for Time Series Classification},
 url = {https://www.mdpi.com/2079-9292/14/7/1311},
 volume = {14},
 year = {2025}
}
